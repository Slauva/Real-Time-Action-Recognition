{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings ignoring\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# OS tools\n",
    "import os\n",
    "import typing\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "# Tables, arrays, and plotters \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "# Video Processing\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as tt\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformers\n",
    "from transformers import TimesformerModel\n",
    "\n",
    "# Lighting\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, strategies\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.utilities import grad_norm\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=16, embed_dim=768):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        \n",
    "        self.proj = nn.Conv3d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=(1, patch_size, patch_size),\n",
    "            stride=(1, patch_size, patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T, H, W)\n",
    "        x = self.proj(x)  # (B, embed_dim, T, H/patch, W/patch)\n",
    "        x = rearrange(x, 'b c t h w -> b (t h w) c')\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x)  # (B, N, 3C)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # Each: (B, heads, N, dim)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, heads, N, N)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v)  # (B, heads, N, dim)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)  # (B, N, C)\n",
    "        return self.proj(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class TimeSformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, num_frames=8, in_channels=3, embed_dim=768, depth=12, num_heads=12, num_classes=400):\n",
    "        super(TimeSformer, self).__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(in_channels, patch_size, embed_dim)\n",
    "        num_patches = (img_size // patch_size) ** 2 * num_frames\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(0.1)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        if self.head.bias is not None:\n",
    "            nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T, H, W)\n",
    "        x = self.patch_embed(x)  # (B, N, C)\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, C)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (B, N+1, C)\n",
    "        x = x + self.pos_embed[:, :N+1]\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        return self.head(x[:, 0])  # class token\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'33,830,501'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TimeSformer(\n",
    "    num_classes=101,\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    num_frames=32,\n",
    "    in_channels=3,\n",
    "    embed_dim=768,\n",
    "    depth=4,\n",
    "    num_heads=4,    \n",
    ")\n",
    "f\"{count_trainable_parameters(net):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 101])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(8, 3, 32, 224, 224)\n",
    "net(x).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
