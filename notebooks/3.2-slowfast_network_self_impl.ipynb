{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings ignoring\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# OS tools\n",
    "import os\n",
    "import typing\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "# Tables, arrays, and plotters \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "# Video Processing\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.models.video import r3d_18\n",
    "\n",
    "# Lighting\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.utilities import grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tensor_size_conv(size, model):\n",
    "    t_size = np.copy(size)\n",
    "    for param in model.children():\n",
    "        if hasattr(param, \"weight\") and hasattr(param, \"kernel_size\"):\n",
    "            k, s, p = param.kernel_size, param.stride, param.padding\n",
    "            for i in range(len(size)):\n",
    "                t_size[i] = np.floor((t_size[i] + 2 * p[i] - k[i]) / s[i] + 1)\n",
    "    return t_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.0)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(27/4 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFusion3D(nn.Module):\n",
    "    def __init__(self, x_channels: int, y_channels: int, embed_channels: int):\n",
    "        super(AttentionFusion3D, self).__init__()\n",
    "        \n",
    "        # Project both paths to same shape if needed\n",
    "        self.align = nn.Conv3d(y_channels, x_channels, kernel_size=(5, 1, 1), stride=(8, 1, 1)) if y_channels != x_channels else nn.Identity()\n",
    "\n",
    "        # Attention mechanism (Conv3D version)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv3d(x_channels * 2, embed_channels, kernel_size=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv3d(embed_channels, 2, kernel_size=1),  # Output: attention map for x and y\n",
    "            nn.Softmax(dim=1)  # Softmax over 2 branches (not channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, C, T, H, W]\n",
    "        y: [B, C' or C, T, H, W]\n",
    "        \"\"\"\n",
    "        y_aligned = self.align(y)  # Ensure both have same channel dim\n",
    "\n",
    "        # Concatenate along channel dimension: [B, 2C, T, H, W]\n",
    "        concat = torch.cat([x, y_aligned], dim=1)\n",
    "\n",
    "        # Compute attention weights: [B, 2, T, H, W]\n",
    "        weights = self.attention(concat)\n",
    "\n",
    "        # Split weights: [B, 1, T, H, W] for x and y\n",
    "        wx = weights[:, 0:1, :, :, :]\n",
    "        wy = weights[:, 1:2, :, :, :]\n",
    "\n",
    "        # Weighted sum\n",
    "        fused = x * wx + y_aligned * wy\n",
    "        return fused\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        config: list[dict],\n",
    "        depth: int = 1,\n",
    "        inplace: bool=False\n",
    "    ):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self._consruct(config, depth, inplace)\n",
    "    \n",
    "    def _consruct(self, config, depth, inplace) -> None:\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(depth):\n",
    "            for j, params in enumerate(config):\n",
    "                layers.extend([\n",
    "                    nn.Conv3d(\n",
    "                        in_channels=self.in_dim,\n",
    "                        out_channels=self.in_dim if j != len(config) - 1 else self.out_dim,\n",
    "                        kernel_size=params.get(\"kernel\", 1),\n",
    "                        stride=params.get(\"stride\", 1),\n",
    "                        padding=params.get(\"padding\", 0),\n",
    "                        bias=False\n",
    "                    ),\n",
    "                    nn.BatchNorm3d(\n",
    "                        num_features=self.in_dim if j != len(config) - 1 else self.out_dim\n",
    "                    ),\n",
    "                    nn.ReLU(inplace=inplace)\n",
    "                ])\n",
    "            \n",
    "            if i != depth - 1:\n",
    "                layers.append(\n",
    "                    nn.Conv3d(self.out_dim, self.in_dim, kernel_size=1, stride=1),\n",
    "                )\n",
    "        self.hidden = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.hidden(x)\n",
    "\n",
    "class SlowFastNet(nn.Module):\n",
    "    def __init__(self, n_outputs: int, blocks: tuple[int, int, int, int]=(3, 4, 6, 3)):\n",
    "        super(SlowFastNet, self).__init__()\n",
    "        \n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        \n",
    "        fn_downsampling = lambda x: nn.Conv3d(x[0], x[1], kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        # Fusions\n",
    "        \n",
    "        self.fusion_1 = AttentionFusion3D(64, 8, 64)\n",
    "        self.fusion_2 = AttentionFusion3D(256, 32, 256)\n",
    "        self.fusion_3 = AttentionFusion3D(512, 64, 512)\n",
    "        self.fusion_4 = AttentionFusion3D(1024, 128, 1024)\n",
    "        self.fusion_5 = AttentionFusion3D(2048, 256, 2048)\n",
    "        \n",
    "        # SlowPathWay\n",
    "        \n",
    "        self.slow_conv_pool_1 = nn.Sequential(\n",
    "            nn.Conv3d(3, 64, kernel_size=(1, 7, 7), stride=(1, 1, 1), padding=(0, 2, 2)),\n",
    "            nn.Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 2, 2), padding=(0, 1, 1)),\n",
    "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1)),\n",
    "            nn.Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 2, 2), padding=(0, 1, 1))\n",
    "        )\n",
    "        \n",
    "        self.slow_res2 = ResidualBlock(\n",
    "            in_dim=64, \n",
    "            out_dim=256, \n",
    "            config=[\n",
    "                {\"kernel\": (1, 1, 1), \"padding\": (0, 0, 0)},\n",
    "                {\"kernel\": (1, 3, 3), \"padding\": (0, 0, 0)},\n",
    "                {\"kernel\": (1, 1, 1), \"padding\": (0, 1, 1)},\n",
    "            ],\n",
    "            depth=blocks[0]\n",
    "        )\n",
    "        \n",
    "        self.slow_downsampling_2 = fn_downsampling([256, 128])\n",
    "        \n",
    "        self.slow_res_3 = ResidualBlock(\n",
    "            in_dim=128, \n",
    "            out_dim=512, \n",
    "            config=[\n",
    "                {\"kernel\": (1, 1, 1), \"padding\": (0, 13, 13), \"stride\": (1, 2, 2)},\n",
    "                {\"kernel\": (1, 3, 3), \"padding\": (0, 0, 0)},\n",
    "                {\"kernel\": (1, 1, 1), \"padding\": (0, 1, 1)},\n",
    "            ],\n",
    "            depth=blocks[1]\n",
    "        )\n",
    "        \n",
    "        self.slow_downsampling_3 = fn_downsampling([512, 256])\n",
    "        \n",
    "        self.slow_res_4 = ResidualBlock(\n",
    "            in_dim=256, \n",
    "            out_dim=1024, \n",
    "            config=[\n",
    "                {\"kernel\": (3, 1, 1), \"padding\": (1, 7, 7), \"stride\": (1, 3, 3)},\n",
    "                {\"kernel\": (1, 3, 3), \"padding\": (0, 2, 2)},\n",
    "                {\"kernel\": (1, 1, 1), \"padding\": (0, 1, 1)},\n",
    "            ],\n",
    "            depth=blocks[2]\n",
    "        )\n",
    "        \n",
    "        self.slow_downsampling_4 = fn_downsampling([1024, 512])\n",
    "        \n",
    "        self.slow_res_5 = ResidualBlock(\n",
    "            in_dim=512, \n",
    "            out_dim=2048, \n",
    "            config=[\n",
    "                {\"kernel\": (3, 1, 1), \"padding\": (1, 8, 8), \"stride\": (1, 5, 5)},\n",
    "                {\"kernel\": (1, 3, 3), \"padding\": (0, 1, 1)},\n",
    "                {\"kernel\": (1, 1, 1), \"padding\": (0, 1, 1)},\n",
    "            ],\n",
    "            depth=blocks[3]\n",
    "        )\n",
    "        \n",
    "        # FastPathWay\n",
    "        \n",
    "        self.fast_conv_pool_1 = nn.Sequential(\n",
    "            nn.Conv3d(3, 8, kernel_size=(1, 7, 7), stride=(1, 1, 1), padding=(0, 2, 2)),\n",
    "            nn.Conv3d(8, 8, kernel_size=(1, 1, 1), stride=(1, 2, 2), padding=(0, 1, 1)),\n",
    "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1)),\n",
    "            nn.Conv3d(8, 8, kernel_size=(1, 1, 1), stride=(1, 2, 2), padding=(0, 1, 1))\n",
    "        )\n",
    "        \n",
    "        self.fast_res2 = ResidualBlock(\n",
    "            in_dim=8, \n",
    "            out_dim=32, \n",
    "            config=[\n",
    "                {\"kernel\": (3, 1, 1), \"padding\": (1, 0, 0)},\n",
    "                {\"kernel\": (1, 3, 3), \"padding\": (0, 0, 0)},\n",
    "                {\"kernel\": (1, 1, 1), \"padding\": (0, 1, 1)},\n",
    "            ],\n",
    "            depth=blocks[0]\n",
    "        )\n",
    "        \n",
    "        self.fast_downsampling_2 = fn_downsampling([32, 16])\n",
    "        \n",
    "        self.fast_res_3 = ResidualBlock(\n",
    "            in_dim=16, \n",
    "            out_dim=64, \n",
    "            config=[\n",
    "                {\"kernel\": (3, 1, 1), \"padding\": (1, 13, 13), \"stride\": (1, 2, 2)},\n",
    "                {\"kernel\": (1, 3, 3), \"padding\": (0, 0, 0)},\n",
    "                {\"kernel\": (1, 1, 1), \"padding\": (0, 1, 1)},\n",
    "            ],\n",
    "            depth=blocks[1]\n",
    "        )\n",
    "        \n",
    "        self.fast_downsampling_3 = fn_downsampling([64, 32])\n",
    "        \n",
    "        self.fast_res_4 = ResidualBlock(\n",
    "            in_dim=32, \n",
    "            out_dim=128, \n",
    "            config=[\n",
    "                {\"kernel\": (3, 1, 1), \"padding\": (1, 7, 7), \"stride\": (1, 3, 3)},\n",
    "                {\"kernel\": (1, 3, 3), \"padding\": (0, 2, 2)},\n",
    "                {\"kernel\": (1, 1, 1), \"padding\": (0, 1, 1)},\n",
    "            ],\n",
    "            depth=blocks[2]\n",
    "        )\n",
    "        \n",
    "        self.fast_downsampling_4 = fn_downsampling([128, 64])\n",
    "        \n",
    "        self.fast_res_5 = ResidualBlock(\n",
    "            in_dim=64, \n",
    "            out_dim=256, \n",
    "            config=[\n",
    "                {\"kernel\": (3, 1, 1), \"padding\": (1, 8, 8), \"stride\": (1, 5, 5)},\n",
    "                {\"kernel\": (1, 3, 3), \"padding\": (0, 1, 1)},\n",
    "                {\"kernel\": (1, 1, 1), \"padding\": (0, 1, 1)},\n",
    "            ],\n",
    "            depth=blocks[3]\n",
    "        )\n",
    "        \n",
    "        # Classification\n",
    "        self.avg_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=2048, out_features=n_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_sl = x[:, :, ::8, :, :]\n",
    "        \n",
    "        # Conv Pool 1\n",
    "        x = self.fast_conv_pool_1(x)\n",
    "        x_sl = self.slow_conv_pool_1(x_sl)\n",
    "        x_sl = self.fusion_1(x_sl, x)\n",
    "        \n",
    "        # Res 2\n",
    "        x = self.fast_res2(x)\n",
    "        x_sl = self.slow_res2(x_sl)\n",
    "        x_sl = self.fusion_2(x_sl, x)\n",
    "        \n",
    "        # Downsampling 2\n",
    "        x = self.fast_downsampling_2(x)\n",
    "        x_sl = self.slow_downsampling_2(x_sl)\n",
    "        \n",
    "        # Res 3\n",
    "        x = self.fast_res_3(x)\n",
    "        x_sl = self.slow_res_3(x_sl)\n",
    "        x_sl = self.fusion_3(x_sl, x)\n",
    "        \n",
    "        # Downsampling 3\n",
    "        x = self.fast_downsampling_3(x)\n",
    "        x_sl = self.slow_downsampling_3(x_sl)\n",
    "        \n",
    "        # Res 4\n",
    "        x = self.fast_res_4(x)\n",
    "        x_sl = self.slow_res_4(x_sl)\n",
    "        x_sl = self.fusion_4(x_sl, x)\n",
    "        \n",
    "        # Downsampling 4\n",
    "        x = self.fast_downsampling_4(x)\n",
    "        x_sl = self.slow_downsampling_4(x_sl)\n",
    "        \n",
    "        # Res 5\n",
    "        x = self.fast_res_5(x)\n",
    "        x_sl = self.slow_res_5(x_sl)\n",
    "        \n",
    "        # Concat\n",
    "        z = self.fusion_5(x_sl, x)\n",
    "        z = self.avg_pool(z)\n",
    "        \n",
    "        return z\n",
    "\n",
    "def slowfast_r18(n_outputs: int) -> nn.Module:\n",
    "    return SlowFastNet(\n",
    "        n_outputs=n_outputs,\n",
    "        blocks=(1, 1, 1, 1)\n",
    "    )\n",
    "\n",
    "def slowfast_r50(n_outputs: int) -> nn.Module:\n",
    "    return SlowFastNet(\n",
    "        n_outputs=n_outputs,\n",
    "        blocks=(3, 4, 6, 3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'39,601,551'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = slowfast_r50(101)\n",
    "x = torch.rand((8, 3, 32, 224, 224))\n",
    "out = net(x)\n",
    "f\"{count_trainable_parameters(net):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion3D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel: int = None,\n",
    "        stride: int = None,\n",
    "    ):\n",
    "        \"\"\"Fusion 3D tensors\n",
    "\n",
    "        Fusion tensor with different channel and resolution sizes. When we call it, the `x` is source tensor [B, T, C, H, W], which should to\n",
    "        be projected to `y` tensor shape [B, T', C', H, W].\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The source channel number\n",
    "            out_channels (int): The target channel number\n",
    "            kernel (int, optional): The kernel size to projected the tensors resolutions\n",
    "            stride (int, optional): The stride to projected the tensors resolutions\n",
    "        \"\"\"\n",
    "        super(Fusion3D, self).__init__()\n",
    "\n",
    "        self.proj_channels = (\n",
    "            nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n",
    "            if in_channels != out_channels\n",
    "            else nn.Identity()\n",
    "        )\n",
    "        self.proj_resolution = (\n",
    "            nn.Conv3d(\n",
    "                in_channels=out_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=(kernel, 1, 1),\n",
    "                stride=(stride, 1, 1),\n",
    "            )\n",
    "            if kernel is not None and stride is not None\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, y: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Fusion 3D tensors\n",
    "\n",
    "        Fusion tensor with different channel and resolution sizes. When we call it, the `x` is source tensor [B, T, C, H, W], which should to\n",
    "        be projected to `y` tensor shape [B, T', C', H, W].\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The source tensor with shape [B, T, C, H, W]\n",
    "            y (Tensor): The target tensor with shape [B, T', C', H, W]\n",
    "\n",
    "        Returns:\n",
    "            out (Tuple[Tensor, Tensor]): The concated tensors with shape [B, T', 2C', H, W], and projectiled source tensor with shape [B, T', C', H, W]\n",
    "        \"\"\"\n",
    "        projected = self.proj_channels(x)\n",
    "        projected = self.proj_resolution(projected)\n",
    "\n",
    "        z = torch.cat([y, projected], dim=1)\n",
    "\n",
    "        return z, projected\n",
    "\n",
    "\n",
    "class AttentionFusion3D(Fusion3D):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        embed: int,\n",
    "        kernel: int = None,\n",
    "        stride: int = None,\n",
    "    ):\n",
    "        super().__init__(in_channels, out_channels, kernel, stride)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv3d(out_channels * 2, embed, kernel_size=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv3d(embed, 2, kernel_size=1),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self, x: torch.Tensor, y: torch.Tensor\n",
    "    ) -> torch.Tensor: \n",
    "        z, projected = super().forward(x, y)\n",
    "        weights = self.attention(z)\n",
    "        \n",
    "        # Split weights: [B, 1, T, H, W] for x and y\n",
    "        wx = weights[:, 0:1, :, :, :]\n",
    "        wy = weights[:, 1:2, :, :, :]\n",
    "\n",
    "        # Weighted sum\n",
    "        fused = y * wx + projected * wy\n",
    "        return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 4, 224, 224])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast = torch.rand((8, 8, 32, 224, 224))\n",
    "slow = torch.rand((8, 32, 4, 224, 224))\n",
    "\n",
    "fusion = AttentionFusion3D(8, 32, 32, 5, 8)\n",
    "\n",
    "fusion(fast, slow).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
