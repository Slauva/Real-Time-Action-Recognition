{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings ignoring\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# OS tools\n",
    "import os\n",
    "import typing\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "# Tables, arrays, and plotters \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "# Video Processing\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "# Lighting\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.utilities import grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HParams:\n",
    "    \n",
    "    # Base parameters\n",
    "    \n",
    "    dataset_dir: Path = Path(\"../data/UCF101\")\n",
    "    \"\"\"Dataset directory path\"\"\"\n",
    "    train_meta: Path = Path(\"../data/UCF101/train.csv\")\n",
    "    \"\"\"Path to meta csv file inforamation for Train Loop\"\"\"\n",
    "    test_meta: Path = Path(\"../data/UCF101/test.csv\")\n",
    "    \"\"\"Path to meta csv file inforamation for Test Loop\"\"\"\n",
    "    validation_meta: Path = Path(\"../data/UCF101/val.csv\")\n",
    "    \"\"\"Path to meta csv file inforamation for Validation Loop\"\"\"\n",
    "    output_dir: Path = Path(\"saved_models/\")\n",
    "    \"\"\"Path to save all output information\"\"\"\n",
    "    \n",
    "    # Dataset parameters\n",
    "    \n",
    "    size: typing.Tuple[int, int] = (224, 224)\n",
    "    \"\"\"Image size [H, W]\"\"\"\n",
    "    mean: typing.Tuple[float, float, float] = (0.485, 0.456, 0.406)\n",
    "    \"\"\"Image normalization parameter: mean\"\"\"\n",
    "    std: typing.Tuple[float, float, float] = (0.229, 0.224, 0.225)\n",
    "    \"\"\"Image normalization parameter: std\"\"\"\n",
    "    clip_len: int = 32\n",
    "    \"\"\"Video frame count [T]\"\"\"\n",
    "    clip_format: str = \"CTHW\"\n",
    "    \"\"\"Final frame shape like: \"TCHW\" \"\"\"\n",
    "    batch_size: int = 8\n",
    "    \"\"\"Batch size [B]\"\"\"\n",
    "    num_workers: int = 2\n",
    "    \"\"\"Workers number\"\"\"\n",
    "    \n",
    "    # Model parameters\n",
    "    \n",
    "    arch: str = \"Some name\"\n",
    "    \"\"\"Archetecture model name (meta info)\"\"\"\n",
    "    n_classes: int = 101\n",
    "    \"\"\"Number of classes\"\"\"\n",
    "    freeze: bool = True\n",
    "    \"\"\"Set requires_grad to False for part of the model, to learn only model head\"\"\"\n",
    "    lr: float = 1e-3\n",
    "    \"\"\"Learning rate\"\"\"\n",
    "    ls: float = 0.4\n",
    "    \"\"\"Label smoothing\"\"\"\n",
    "    weight_decay: float = .0\n",
    "    \"\"\"Optimizer param weight_decay\"\"\"\n",
    "    num_epoch: int = 10\n",
    "    \"\"\"Number of epoch\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_dir': PosixPath('../data/UCF101'),\n",
       " 'train_meta': PosixPath('../data/UCF101/train.csv'),\n",
       " 'test_meta': PosixPath('../data/UCF101/test.csv'),\n",
       " 'validation_meta': PosixPath('../data/UCF101/val.csv'),\n",
       " 'output_dir': PosixPath('saved_models'),\n",
       " 'size': (224, 224),\n",
       " 'mean': (0.485, 0.456, 0.406),\n",
       " 'std': (0.229, 0.224, 0.225),\n",
       " 'clip_len': 32,\n",
       " 'clip_format': 'CTHW',\n",
       " 'batch_size': 8,\n",
       " 'num_workers': 2,\n",
       " 'arch': 'Some name',\n",
       " 'n_classes': 101,\n",
       " 'freeze': True,\n",
       " 'lr': 0.001,\n",
       " 'ls': 0.4,\n",
       " 'weight_decay': 0.0,\n",
       " 'num_epoch': 10}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HParams().__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dir: Path, meta: Path, clip_len: int, transform: v2.Transform = None, output_format: str = \"TCHW\") -> None:\n",
    "        \"\"\" Dataset class to load UCF101\n",
    "        \n",
    "        Args:\n",
    "            dir (Path): Path to the directory with video files.\n",
    "            meta (Path): Path to file with information of video [clip_name, clip_path, label] in csv format\n",
    "            clip_len (int): The number of frames per video\n",
    "            transform (Transform, optional): Optional transform to be applied on a sample\n",
    "            output_format (str, optional): The format of the output video tensors. Can be either \"TCHW\" (default) or differ combination.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dir = dir\n",
    "        self.clip_len = clip_len\n",
    "        self.transform = transform\n",
    "        self.output_format = output_format\n",
    "        \n",
    "        df = pd.read_csv(meta)\n",
    "        \n",
    "        labels = sorted(df[\"label\"].unique())\n",
    "        \n",
    "        self._map_label2idx = {l:i for i, l in enumerate(labels)}\n",
    "        self._map_idx2label = {i:l for i, l in enumerate(labels)}\n",
    "        \n",
    "        self.labels = df[\"label\"].to_numpy()\n",
    "        self.paths = df[\"clip_path\"].to_numpy()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _clip_sampler(self, frames: torch.Tensor) -> torch.Tensor:\n",
    "        if frames.shape[0] < self.clip_len:\n",
    "            padding_size = self.clip_len - frames.shape[0]\n",
    "            last_frame = frames[-1].unsqueeze(0)\n",
    "            padded_video = torch.cat([frames, last_frame.repeat(padding_size, 1, 1, 1)], dim=0)\n",
    "            return padded_video\n",
    "        else:\n",
    "            padding_size = frames.shape[0] - self.clip_len \n",
    "            start_idx = np.random.randint(0, padding_size + 1)\n",
    "            return frames[start_idx:start_idx + self.clip_len]\n",
    "    \n",
    "    def _clip_format(self, frames: torch.Tensor) -> torch.Tensor:\n",
    "        f_idx = {\"T\": 0, \"C\": 1, \"H\": 2, \"W\": 3}\n",
    "        transpose_idx = [f_idx[i] for i in self.output_format]\n",
    "        return frames.permute(*transpose_idx)\n",
    "    \n",
    "    def __getitem__(self, idx) -> typing.Tuple[torch.Tensor, int, int]:\n",
    "        label = self.labels[idx]\n",
    "        path = self.paths[idx][1:]\n",
    "        \n",
    "        frames, *_ = read_video(os.path.join(self.dir, path), output_format=\"TCHW\")\n",
    "        frames = frames.float() / 255\n",
    "        frames = self._clip_sampler(frames)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            frames = self.transform(frames)\n",
    "        \n",
    "        frames = self._clip_format(frames)\n",
    "        \n",
    "        return frames, self._map_label2idx[label], idx        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, params: HParams) -> None:\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        \n",
    "        self.transform = v2.Compose([\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize(size=params.size),\n",
    "            v2.Normalize(mean=params.mean, std=params.std)\n",
    "        ])\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.train = VideoDataset(\n",
    "            self.params.dataset_dir,\n",
    "            self.params.train_meta,\n",
    "            self.params.clip_len,\n",
    "            self.transform,\n",
    "            self.params.clip_format\n",
    "        )\n",
    "        \n",
    "        self.test = VideoDataset(\n",
    "            self.params.dataset_dir,\n",
    "            self.params.test_meta,\n",
    "            self.params.clip_len,\n",
    "            self.transform,\n",
    "            self.params.clip_format\n",
    "        )\n",
    "        \n",
    "        self.validation = VideoDataset(\n",
    "            self.params.dataset_dir,\n",
    "            self.params.validation_meta,\n",
    "            self.params.clip_len,\n",
    "            self.transform,\n",
    "            self.params.clip_format\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train,\n",
    "            batch_size=self.params.batch_size,\n",
    "            num_workers=self.params.num_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.validation,\n",
    "            batch_size=self.params.batch_size,\n",
    "            num_workers=self.params.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test,\n",
    "            batch_size=self.params.batch_size,\n",
    "            num_workers=self.params.num_workers,\n",
    "            shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_model(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module=None, params: HParams=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params.__dict__, ignore=(\"dataset_dir\", \"train_meta\", \"test_meta\", \"validation_meta\"))\n",
    "        self.params = params\n",
    "        self.model = model\n",
    "        \n",
    "        self.accuracy = F1Score(task=\"multiclass\", num_classes=params.n_classes, average=\"micro\")\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=params.ls)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        \n",
    "        self.log_dict({\n",
    "            \"train_loss\": loss,\n",
    "            \"train_acc\": acc,\n",
    "        }, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        \n",
    "        self.log_dict({\n",
    "            \"val_loss\": loss,\n",
    "            \"val_acc\": acc,\n",
    "        }, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "\n",
    "        self.log_dict({\n",
    "            \"test_loss\": loss,\n",
    "            \"test_acc\": acc,\n",
    "        }, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=self.params.lr, \n",
    "            weight_decay=self.params.weight_decay\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.0)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "        norm_order = 2.0\n",
    "        norms = grad_norm(self, norm_type=norm_order)\n",
    "        self.log(\"grad_norm\", norms[f'grad_{norm_order}_norm_total'], on_step=True, on_epoch=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
