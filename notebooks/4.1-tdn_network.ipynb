{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings ignoring\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# OS tools\n",
    "import os\n",
    "import typing\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "# Tables, arrays, and plotters \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "# Video Processing\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as tt\n",
    "import torchvision.models as models\n",
    "\n",
    "# Lighting\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, strategies\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.utilities import grad_norm\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDifferenceModule(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, in_channels, kernel_size=(3, 1, 1), padding=(1, 0, 0), groups=in_channels)\n",
    "        self.bn = nn.BatchNorm3d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, C, T, H, W]\n",
    "        diff = x[:, :, 1:, :, :] - x[:, :, :-1, :, :]  # [B, C, T-1, H, W]\n",
    "        diff = torch.nn.functional.pad(diff, (0, 0, 0, 0, 1, 0))  # pad time dim to match input\n",
    "        out = self.relu(self.bn(self.conv(diff)))\n",
    "        return out + x  # residual connection\n",
    "\n",
    "\n",
    "class TDN(nn.Module):\n",
    "    def __init__(self, num_classes=100, backbone_name='resnet50'):\n",
    "        super().__init__()\n",
    "        # Load 2D ResNet and adapt it for video\n",
    "        resnet2d = models.resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet2d.children())[:-2])  # remove avgpool & fc\n",
    "        \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.tdm = TemporalDifferenceModule(in_channels=2048)  # final ResNet feature channels\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, T, C, H, W]\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        feat = self.backbone(x)  # [B*T, C, H', W']\n",
    "        _, C2, H2, W2 = feat.shape\n",
    "        feat = feat.view(B, T, C2, H2, W2).permute(0, 2, 1, 3, 4)  # [B, C, T, H, W]\n",
    "\n",
    "        feat = self.tdm(feat)  # [B, C, T, H, W]\n",
    "        out = self.pool(feat).flatten(1)  # [B, C]\n",
    "        return self.fc(out)  # logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HParams:\n",
    "    \n",
    "    # Base parameters\n",
    "    \n",
    "    dataset_dir: Path = Path(\"../data/UCF101\")\n",
    "    \"\"\"Dataset directory path\"\"\"\n",
    "    train_meta: Path = Path(\"../data/UCF101/test.csv\")\n",
    "    \"\"\"Path to meta csv file inforamation for Train Loop\"\"\"\n",
    "    test_meta: Path = Path(\"../data/UCF101/test.csv\")\n",
    "    \"\"\"Path to meta csv file inforamation for Test Loop\"\"\"\n",
    "    validation_meta: Path = Path(\"../data/UCF101/val.csv\")\n",
    "    \"\"\"Path to meta csv file inforamation for Validation Loop\"\"\"\n",
    "    output_dir: Path = Path(\"saved_models/\")\n",
    "    \"\"\"Path to save all output information\"\"\"\n",
    "    \n",
    "    # Dataset parameters\n",
    "    \n",
    "    size: typing.Tuple[int, int] = (224, 224)\n",
    "    \"\"\"Image size [H, W]\"\"\"\n",
    "    mean: typing.Tuple[float, float, float] = (0.485, 0.456, 0.406)\n",
    "    \"\"\"Image normalization parameter: mean\"\"\"\n",
    "    std: typing.Tuple[float, float, float] = (0.229, 0.224, 0.225)\n",
    "    \"\"\"Image normalization parameter: std\"\"\"\n",
    "    clip_len: int = 32\n",
    "    \"\"\"Video frame count [T]\"\"\"\n",
    "    clip_format: str = \"CTHW\"\n",
    "    \"\"\"Final frame shape like: \"TCHW\" \"\"\"\n",
    "    batch_size: int = 8\n",
    "    \"\"\"Batch size [B]\"\"\"\n",
    "    num_workers: int = 2\n",
    "    \"\"\"Workers number\"\"\"\n",
    "    \n",
    "    # Model parameters\n",
    "    \n",
    "    arch: str = \"timesformer\"\n",
    "    \"\"\"Archetecture model name (meta info)\"\"\"\n",
    "    n_classes: int = 101\n",
    "    \"\"\"Number of classes\"\"\"\n",
    "    freeze: bool = True\n",
    "    \"\"\"Set requires_grad to False for part of the model, to learn only model head\"\"\"\n",
    "    lr: float = 1e-3\n",
    "    \"\"\"Learning rate\"\"\"\n",
    "    ls: float = 0.4\n",
    "    \"\"\"Label smoothing\"\"\"\n",
    "    weight_decay: float = .0\n",
    "    \"\"\"Optimizer param weight_decay\"\"\"\n",
    "    num_epoch: int = 10\n",
    "    \"\"\"Number of epoch\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dir: Path, meta: Path, clip_len: int, transform: v2.Transform = None, output_format: str = \"TCHW\") -> None:\n",
    "        \"\"\" Dataset class to load UCF101\n",
    "        \n",
    "        Args:\n",
    "            dir (Path): Path to the directory with video files.\n",
    "            meta (Path): Path to file with information of video [clip_name, clip_path, label] in csv format\n",
    "            clip_len (int): The number of frames per video\n",
    "            transform (Transform, optional): Optional transform to be applied on a sample\n",
    "            output_format (str, optional): The format of the output video tensors. Can be either \"TCHW\" (default) or differ combination.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dir = dir\n",
    "        self.clip_len = clip_len\n",
    "        self.transform = transform\n",
    "        self.output_format = output_format\n",
    "        \n",
    "        df = pd.read_csv(meta)\n",
    "        \n",
    "        labels = sorted(df[\"label\"].unique())\n",
    "        \n",
    "        self._map_label2idx = {l:i for i, l in enumerate(labels)}\n",
    "        self._map_idx2label = {i:l for i, l in enumerate(labels)}\n",
    "        \n",
    "        self.labels = df[\"label\"].to_numpy()\n",
    "        self.paths = df[\"clip_path\"].to_numpy()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _clip_sampler(self, frames: torch.Tensor) -> torch.Tensor:\n",
    "        if frames.shape[0] < self.clip_len:\n",
    "            padding_size = self.clip_len - frames.shape[0]\n",
    "            last_frame = frames[-1].unsqueeze(0)\n",
    "            padded_video = torch.cat([frames, last_frame.repeat(padding_size, 1, 1, 1)], dim=0)\n",
    "            return padded_video\n",
    "        else:\n",
    "            padding_size = frames.shape[0] - self.clip_len \n",
    "            start_idx = np.random.randint(0, padding_size + 1)\n",
    "            return frames[start_idx:start_idx + self.clip_len]\n",
    "    \n",
    "    def _clip_format(self, frames: torch.Tensor) -> torch.Tensor:\n",
    "        f_idx = {\"T\": 0, \"C\": 1, \"H\": 2, \"W\": 3}\n",
    "        transpose_idx = [f_idx[i] for i in self.output_format]\n",
    "        return frames.permute(*transpose_idx)\n",
    "    \n",
    "    def __getitem__(self, idx) -> typing.Tuple[torch.Tensor, int, int]:\n",
    "        label = self.labels[idx]\n",
    "        path = self.paths[idx][1:]\n",
    "        \n",
    "        frames, *_ = read_video(os.path.join(self.dir, path), output_format=\"TCHW\")\n",
    "        frames = frames.float() / 255\n",
    "        frames = self._clip_sampler(frames)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            frames = self.transform(frames)\n",
    "        \n",
    "        frames = self._clip_format(frames)\n",
    "        \n",
    "        return frames, self._map_label2idx[label], idx\n",
    "\n",
    "class VideoDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, params: HParams) -> None:\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        \n",
    "        self.transform = v2.Compose([\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize(size=params.size),\n",
    "            v2.Normalize(mean=params.mean, std=params.std)\n",
    "        ])\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.train = VideoDataset(\n",
    "            self.params.dataset_dir,\n",
    "            self.params.train_meta,\n",
    "            self.params.clip_len,\n",
    "            self.transform,\n",
    "            self.params.clip_format\n",
    "        )\n",
    "        \n",
    "        self.test = VideoDataset(\n",
    "            self.params.dataset_dir,\n",
    "            self.params.test_meta,\n",
    "            self.params.clip_len,\n",
    "            self.transform,\n",
    "            self.params.clip_format\n",
    "        )\n",
    "        \n",
    "        self.validation = VideoDataset(\n",
    "            self.params.dataset_dir,\n",
    "            self.params.validation_meta,\n",
    "            self.params.clip_len,\n",
    "            self.transform,\n",
    "            self.params.clip_format\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train,\n",
    "            batch_size=self.params.batch_size,\n",
    "            num_workers=self.params.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=False\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.validation,\n",
    "            batch_size=self.params.batch_size,\n",
    "            num_workers=self.params.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=False\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test,\n",
    "            batch_size=self.params.batch_size,\n",
    "            num_workers=self.params.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_model(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module=None, params: HParams=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params.__dict__, ignore=(\"dataset_dir\", \"train_meta\", \"test_meta\", \"validation_meta\"))\n",
    "        self.params = params\n",
    "        self.model = model\n",
    "        \n",
    "        self.accuracy = F1Score(task=\"multiclass\", num_classes=params.n_classes, average=\"micro\")\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=params.ls)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        \n",
    "        self.log_dict({\n",
    "            \"train_loss\": loss,\n",
    "            \"train_acc\": acc,\n",
    "        }, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        \n",
    "        self.log_dict({\n",
    "            \"val_loss\": loss,\n",
    "            \"val_acc\": acc,\n",
    "        }, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "\n",
    "        self.log_dict({\n",
    "            \"test_loss\": loss,\n",
    "            \"test_acc\": acc,\n",
    "        }, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(), \n",
    "            lr=self.params.lr, \n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "        norm_order = 2.0\n",
    "        norms = grad_norm(self, norm_type=norm_order)\n",
    "        self.log(\"grad_norm\", norms[f'grad_{norm_order}_norm_total'], on_step=True, on_epoch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model paramteres: 219,237\n"
     ]
    }
   ],
   "source": [
    "hparams = HParams(\n",
    "    num_workers=2,\n",
    "    clip_format=\"TCHW\",\n",
    "    arch=\"tdn\",\n",
    "    num_epoch=20,\n",
    "    clip_len=32,\n",
    ")\n",
    "data_module = VideoDataModule(hparams)\n",
    "\n",
    "agent = TDN(\n",
    "    num_classes=hparams.n_classes\n",
    ")\n",
    "\n",
    "print(f\"Number of model paramteres: {count_trainable_parameters(agent):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5733), started 0:07:21 ago. (Use '!kill 5733' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f23f9f37319b8c5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f23f9f37319b8c5\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /home/slauva/Documents/innopolis/Computer Vision 2025/final/temp/saved_models/best_model_tdn.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model     | TDN               | 23.7 M | train\n",
      "1 | accuracy  | MulticlassF1Score | 0      | train\n",
      "2 | criterion | CrossEntropyLoss  | 0      | train\n",
      "--------------------------------------------------------\n",
      "219 K     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.7 M    Total params\n",
      "94.909    Total estimated model params size (MB)\n",
      "158       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /home/slauva/Documents/innopolis/Computer Vision 2025/final/temp/saved_models/best_model_tdn.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd50f6c3878488b9d6609f1e93720c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda43054b9224223896939b90551bc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d998222ba84604b49e7a0fcfb4fd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddedace74e4847ca8e2398571bf46259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e55c0123cfd4f689b8842431dde95cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998dc02f39ba4e35b5cdd20b2e112279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf3fd2dcf824c5e808162347005c050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9293adfb0a94df7a4963ba79f5f5562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0aca6c2a5014645923990611a023bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2831e0fac82c4946ac73beb6348a88c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bb6550c060418da7b1fa875bd64deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2981b755a1f844e79f2c9987ec038a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback_img = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=hparams.output_dir,\n",
    "    filename=f\"best_model_{hparams.arch}\",\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "model = train_model(model=agent, params=hparams)\n",
    "\n",
    "logger = TensorBoardLogger(\"logs\", name=hparams.arch)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=hparams.num_epoch,\n",
    "    callbacks=[checkpoint_callback_img],\n",
    "    accelerator=\"auto\", \n",
    "    devices=\"auto\",\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module, ckpt_path=\"/home/slauva/Documents/innopolis/Computer Vision 2025/final/temp/saved_models/best_model_tdn.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/slauva/Documents/innopolis/Computer Vision 2025/final/temp/saved_models/best_model_tdn.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/slauva/Documents/innopolis/Computer Vision 2025/final/temp/saved_models/best_model_tdn.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d23e465efdb4e8b97d17f0c63964ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch         0.9216482639312744\n",
      "     test_loss_epoch         2.958406448364258\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "best_model_path = checkpoint_callback_img.best_model_path\n",
    "info = trainer.test(\n",
    "    model=model,\n",
    "    dataloaders=data_module,\n",
    "    ckpt_path=best_model_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
